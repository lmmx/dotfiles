# Website offline proxy-related
# =============================

# -p = --page-requisites, all the files necessary to display the given page (CSS etc.)
# -k = --convert-links, convert URL links in the document to local URIs
# -r = --recursive, follow and retrieve all linked URLs (up to default depth limit of 5)
# -c = --continue, continue getting partially downloaded files
# -np = --no-parent, never ascend to the parent of the target URL when recursing
alias nprwget='wget -pkrc -np'
alias rwget='wget -pkrc'

# 2021
# ====

function paperdl_view() {
	PAPERDL_TARGET_DIR="$HOME/Downloads/pdfs/ml/2021"
	PAPERDL_START_WDIR=$(pwd)
	cd $PAPERDL_TARGET_DIR
	PAPER_URL="$1"
	# If an arXiv link, switch to PDF export to permit `wget` to download without user-agent headers
	PAPER_URL=$(echo $PAPER_URL \
		| sed -E 's|^(https?://arxiv.org/abs/)(.*)|https://export.arxiv.org/pdf/\2.pdf|' \
		| sed -E 's|^(https?://arxiv.org)(/pdf/.*)|https://export.arxiv.org\2|'
	)
	WGET_OUTPUT_LOG=$(wget --content-disposition "$PAPER_URL" 2>&1)
	WGET_OUTPUT_FILE=$(echo "$WGET_OUTPUT_LOG" | grep -E "^Saving to: " -m 1 | cut -d ' ' -f 3- | sed 's/.$//; s/^.//')
	WGET_OUTPUT_PREFIX=$(basename -s ".pdf" $WGET_OUTPUT_FILE)
	NEW_PAPERDL_DIR="$PAPERDL_TARGET_DIR/$WGET_OUTPUT_PREFIX"
	mkdir "$NEW_PAPERDL_DIR"
	cd $NEW_PAPERDL_DIR
	mv "$PAPERDL_TARGET_DIR/$WGET_OUTPUT_FILE" ./
	pdf2up --all "$WGET_OUTPUT_FILE"
	FIRST_PNG=$(ls *.png | head -1) # Alphabetically first
	xdg-open $FIRST_PNG
}

# Scientific literature-related
# =============================

# NOTE: THIS USES ALIASES FROM bashrc_functions_pdf

# Download a file from biorXiv with a given filename
# Standard crop size is 60, `recropreading()` as needed
# Usage:
#    paperdl http://biorxiv.org/XYZ.pdf filename
# 
#    Where filename is in the format 'surname16_abbrev-title' or 'surname16_abbrev-title.pdf'
#    (.pdf extension will be added automatically if not provided)
function DEPRECATED_paperdl() {
	local wgetflags=''
	# TODO if no filename passed in then ask for it (reuse code)
	if [ -z $2 ]; then >&2 echo "No filename provided"; return 1; fi
	# if filename passed in ends with PDF
	if [ "${2##*.}" = 'pdf' ]; then biorx_file="$2"; else biorx_file="$2.pdf"; fi
	if [ $(URLPARSEpaperdl $1) == 'arxiv.org' ]; then wgetflags="$wgetflags --user-agent=Lynx"; fi
	wget -O $biorx_file $1 $wgetflags
	addreading $biorx_file 60
	>&2 printf "Do you want to take notes on this file now? [Press space to open editR] : "
	read -d'' -s -n1 -r
	if [ "$REPLY" = ' ' ]; then
		# Skip the note creation check as it's a new file
		>&2 echo "\nEditing file now..."
		readnwrite "$biorx_file" "--skip-create-check" # opens markdown editor
		return 0
	fi
	>&2 printf "\nDo you want to open this paper in a PDF viewer? [Press enter to open the default PDF viewer or any other key to finish] : "
	read -d'' -s -n1 -r
	if [ "$REPLY" = $'\x0a' ]; then
		>&2 echo "Opening file in a PDF viewer now..."
		xdg-open "$biorx_file"
		takenote "$biorx_file" "--skip-create-check" # opens markdown editor
	fi
	biorx_note_file=$(findnote "$biorx_file" "--skip-create-check")
	>&2 echo -e "\nDone. File written to $(pathto $biorx_file), note file created at $(pathto $biorx_note_file)."
	return 0
}


# capitalised to avoid tab completion ('internal' function, not for users)
function URLPARSEpaperdl () {
	local URL=$1
	if [ $# -gt 1 ]; then
		>&2 echo "Multiple strings detected by URLPARSEpaperdl function (URLs cannot contain spaces!)"
		return 1
	fi
	python -c "from urllib import parse
if '$URL'.find('://') == -1:
  url = parse.urlparse('//$URL','http')
else:
  url = parse.urlparse('$URL')
print(url.netloc)"
	# echo "$URL"
}
